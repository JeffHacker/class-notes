{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Ling-Spam dataset](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html)\n",
    "  * Preprocessed\n",
    "* [Bag of Words](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "  * CountVectorizer\n",
    "    * binary=True\n",
    "* [Multinomial vs Bernoulli Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "* [20 Newsgroups](http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classification\n",
    "\n",
    "So far we've covered linear regression which is handy for predicting a Y given an X or any multitude of variables. Unfortunately we don't always want to predict an indeterminate outcome given an input - sometimes we will want to find out what category something will fall into.  Spam filters do this all of the time.\n",
    "\n",
    "The way it works is that your spam filter has seen a lot of spam - more than any human should have to - and it's been told what spam looks like and how to find more spam.  Doing this feeds it's hunger for more spam.  The more spam it reads and is validated the better it gets.\n",
    "\n",
    "This is using a Bayesian method of classification where it strengthens (or weakens) it's belief in a probability to be true (or false) given any amount of data.\n",
    "\n",
    "Take our coin flip for example. With an unbiased coin we may get somewhat crazy data with a tiny data set, but with millions of flips - we are able to accurately predict the probability that a heads or tails outcome will occur.\n",
    "\n",
    "In our case, we want to classify a message as spam or not spam given a strong history of learning what spam looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This handy function will help us read in our data quickly. Since we have **a lot** of spam (and not spam) we want to train our prediction models well.  So we want to load in our training spam/not-spam files as well as our testing spam/not-spam files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_ling_spam(directory):\n",
    "    files = glob.glob(\"ling-spam/{}/*.txt\".format(directory))\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(file) as fh:\n",
    "            texts.append(fh.read())\n",
    "    return texts\n",
    "\n",
    "spam_train = read_ling_spam(\"spam-train\")\n",
    "non_spam_train = read_ling_spam(\"nonspam-train\")\n",
    "spam_test = read_ling_spam(\"spam-test\")\n",
    "non_spam_test = read_ling_spam(\"nonspam-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A count vectorizer will tokenize a corpus of text and produce a matrix of counts for the given words.\n",
    "\n",
    "Read more on `CounterVectorizer` [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our vectorizer just the same as we have before with our Linear Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `.fit()` method with our combined non-spam and spam training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(non_spam_train + spam_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and extrapolate our transformed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(non_spam_train + spam_train)\n",
    "y_train = ['non-spam'] * len(non_spam_train) + ['spam'] * len(spam_train)\n",
    "X_test = vectorizer.transform(non_spam_test + spam_test)\n",
    "y_test = ['non-spam'] * len(non_spam_test) + ['spam'] * len(spam_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our X_train creates what is called a `sparse matrix`\n",
    "\n",
    "> In numerical analysis, a sparse matrix is a matrix in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The fraction of zero elements over the total number of elements in a matrix is called the sparsity (density). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y_train will label a corelated message as spam or not spam. Imagine this as a label for a message in the same index on the `X_train` sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[1000:1005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to compare the lengths of our training data vs. the amount of features our vectorizer has created for us we can print the lengths.\n",
    "\n",
    "Notice quite a big difference between the two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(spam_train + non_spam_train), len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter stage right, our Multinomial Naive Bayes Classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit our X and Y data to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOLY BAYES!!!\n",
    "\n",
    "oh wait a second wait, what did we do wrong? <sub>Did you get it?<sub> are you sure? </sub></sub>\n",
    "\n",
    "Yup, you probably did, we passed our training data back to our classifier. WRONNNNNNG! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try again but this time lets pass our TEST data sets into our classifier and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all! 97% accuracy for spam prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when we pass messages in to our prediction function it will give us an answer - and remember, with 97% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline is a simple way to create a transformation pipeline for our data.\n",
    "\n",
    "If you noticed above we had do do a few steps before we were able to adequate score our prediction model.\n",
    "\n",
    " - We had to vectorize the corpus\n",
    " - We had to transform the vectorized data\n",
    " - We had to fit our Multinomian Naive Bayes classifier to our data\n",
    " \n",
    "What if I told you there was a way you could create a pipeline for all of those to be done in one step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_pipe = Pipeline([('bag_of_words', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('bayes', MultinomialNB())])\n",
    "spam_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still require our `y_train` data, if you remember was our spam/no spam labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_pipe.fit(non_spam_train + spam_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after all of that, we can pass our `non_spam_test` data set into our vectorized-transformed-classified model and get back the information we'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_pipe.predict(non_spam_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by scoring our combined spam/non-spam test sets against our Y labels we get......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_pipe.score(non_spam_test + spam_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
