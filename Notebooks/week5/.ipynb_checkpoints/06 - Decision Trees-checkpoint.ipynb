{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict, namedtuple\n",
    "from functools import partial\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are another type of classifier we can use in the same way we can use a Naive Bayes classifier. They work much differently, however.\n",
    "\n",
    "A decision tree is basically a tree of if statements and could be coded by hand, like the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is the type of an animal?\n",
    "# Does it have a backbone?\n",
    "#     If it's cold-blooded:\n",
    "#       - Is it born in the water?\n",
    "#         - Does it have lungs as an adult? Amphibian.\n",
    "#         - Else: Fish.\n",
    "#       - Else: Reptile.\n",
    "#     Else:\n",
    "#       - Does it drink milk as a baby? Mammal.\n",
    "#       - Else: Bird.\n",
    "# Else:\n",
    "#     Arthropod.\n",
    "\n",
    "Animal = namedtuple(\"Animal\", \"has_backbone,cold_blooded,born_in_water,breathes_air,milk_drinker\")\n",
    "\n",
    "crow = Animal(True, False, False, True, False)\n",
    "bass = Animal(True, True, True, False, False)\n",
    "spider = Animal(False, True, False, True, False)\n",
    "rattlesnake = Animal(True, True, False, True, False)\n",
    "\n",
    "def animal_type(animal):\n",
    "    if animal.has_backbone:\n",
    "        if animal.cold_blooded:\n",
    "            if animal.born_in_water:\n",
    "                if animal.breathes_air:\n",
    "                    return \"amphibian\"\n",
    "                else:\n",
    "                    return \"fish\"\n",
    "            else:\n",
    "                return \"reptile\"\n",
    "        else:\n",
    "            if animal.milk_drinker:\n",
    "                return \"mammal\"\n",
    "            else:\n",
    "                return \"bird\"\n",
    "    else:\n",
    "        return \"arthropod\"\n",
    "\n",
    "print(\"bass\", animal_type(bass))\n",
    "print(\"crow\", animal_type(crow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're dealing with a lot of data we want to classify and we don't know how to choose its classification, we can't create this by hand, so we use a decision tree algorithm that can learn from our training data.\n",
    "\n",
    "**NOTE**: A big difference with decision trees compared to Naive Bayes is that it can use categories, not just numbers, as features.\n",
    "\n",
    "A decision tree works by looking at the different features it could split a dataset on, and then it chooses the attribute that produces the least entropy -- that is, it generates the most uniform subset. From here, it continues to partition the dataset until it reaches a subset that is completely uniform or until there's no more features to partition with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Decision Trees\n",
    "\n",
    "To build a decision tree, we'll use an algorithm called the [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm). From the Wikipedia page, here's the summary of the algorithm:\n",
    "\n",
    "1. Calculate the entropy of every attribute using the data set S.\n",
    "2. Split the set S into subsets using the attribute for which entropy is minimum.\n",
    "3. Make a decision tree node containing that attribute.\n",
    "4. Recur on subsets using remaining attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset of poker hands, can we predict/understand the hand of any five cards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\n",
    "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\n",
    "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand.names    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poker_train_X = []\n",
    "poker_train_y = []\n",
    "columns = [\"S1\", \"C1\", \"S2\", \"C2\", \"S3\", \"C3\", \"S4\", \"C4\", \"S5\", \"C5\", \"hand\"] \n",
    "with open(\"poker-hand-training-true.data\") as file:\n",
    "    reader = csv.DictReader(file, fieldnames=columns)\n",
    "    for row in reader:\n",
    "        poker_train_y.append(row.pop(\"hand\"))\n",
    "        poker_train_X.append(row)\n",
    "    \n",
    "print(len(poker_train_X))\n",
    "print(poker_train_X[0], poker_train_y[0])\n",
    "\n",
    "poker_test_X = []\n",
    "poker_test_y = []\n",
    "with open(\"poker-hand-testing.data\") as file:\n",
    "    reader = csv.DictReader(file, fieldnames=columns)\n",
    "    for row in reader:\n",
    "        poker_test_y.append(row.pop(\"hand\"))\n",
    "        poker_test_X.append(row)\n",
    "    \n",
    "print(len(poker_test_X))\n",
    "print(poker_test_X[0], poker_test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = DecisionTree()\n",
    "tree.fit(poker_train_X, poker_train_y)\n",
    "tree.score(poker_test_X[0:1000], poker_test_y[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = DecisionTree()\n",
    "tree.fit(poker_test_X[:250000], poker_test_y[:250000])\n",
    "tree.score(poker_train_X, poker_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn\n",
    "\n",
    "Our DecisionTree class is cool, but it's limited in a few ways. Mainly, it can't handle continuous data. Luckily, the SciKit-Learn version can handle all of that. Let's use it to look at some data about whether to approve credit card applicants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "credit_data = pd.read_csv(\"crx.data\", header=None, na_values=['?'])\n",
    "credit_data.dropna(inplace=True)\n",
    "X = credit_data.loc[:, 0:14]\n",
    "y = credit_data[15]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report you see below has _precision_ and _recall_. Precision is the percentage of instances of that class correctly predicted, while recall is the percentage of predictions for that class that are correct.\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall:\n",
    "\n",
    "$$\n",
    "2 \\times \\frac{p \\times r}{p + r}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(tree.predict(X_test), y_test))\n",
    "confusion_matrix(tree.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining our tree\n",
    "\n",
    "We can visualize our decision tree, which can be helpful or just interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree, out_file='tree.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!dot tree.dot -Tpng -o tree.png\n",
    "!open tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try the poker hands again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "tree = make_pipeline(DictVectorizer(), DecisionTreeClassifier())\n",
    "tree.fit(poker_train_X, poker_train_y)\n",
    "tree.score(poker_test_X[0:1000], poker_test_y[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, our version is better than this!\n",
    "\n",
    "This is likely because we use a different algorithm to create our tree. The default algorithm for determining partitions in the SciKit-Learn decision tree is the \"Gini impurity,\" which is \"is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset.\" ([Gini Impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifiers\n",
    "\n",
    "A _random forest classifier_ makes multiple decision trees, each leaving out some of the available features. When classifying, it asks all the decision trees for their vote and chooses the highest voted option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tree = make_pipeline(DictVectorizer(), RandomForestClassifier())\n",
    "tree.fit(poker_train_X, poker_train_y)\n",
    "tree.score(poker_test_X[0:1000], poker_test_y[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Make a random forest classifier for our DecisionTree class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, trees=5, features=0.75):\n",
    "        self.tree_count = trees\n",
    "        self.feature_percent = features\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Create `tree_count` trees, each with feature count of \n",
    "        `feature_percentage` % of features.\"\"\"\n",
    "        self.trees = []\n",
    "        features = list(X[0].keys())\n",
    "        num_features = max(1, int(len(features) * self.feature_percent))\n",
    "        for _ in range(self.tree_count):\n",
    "            self.trees.append(build_tree_id3(list(zip(X, y)),\n",
    "                                             random.sample(features, num_features)))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        for x in X:\n",
    "            votes = Counter([classify(tree, x) for tree in self.trees])\n",
    "            results.append(votes.most_common()[0][0])\n",
    "        return results\n",
    "            \n",
    "            \n",
    "    def score(self, X, y):\n",
    "        results = self.predict(X)\n",
    "        correct = 0\n",
    "        for idx in range(len(y)):\n",
    "            if y[idx] == results[idx]:\n",
    "                correct += 1\n",
    "        return correct / len(y)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForest()\n",
    "forest.fit(poker_test_X[:250000], poker_test_y[:250000])\n",
    "forest.score(poker_train_X, poker_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
